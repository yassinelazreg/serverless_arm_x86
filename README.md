
# AWS Lambda on ARM and x86
This project contains instructions to run the experiments shown in our work. It also contains the results collected during our experiments (`results` folder) as well as the scripts used for processing and evaluating the results (diagrams, tables, etc.) (`EvaluationScripts`).

## Running the Experiments and Collecting Metrics
To run the experiments, we need the SeBS framework. 

#### 1. Clone the SeBS repository:

    git clone https://github.com/spcl/serverless-benchmarks.git

* Requirements:
    * x86-based machine architecture (SeBS only works on x86-based Linux and other POSIX systems)
    * Docker (at least version 19)
    * Python 3.7+ with pip and venv
    * libcurl and its headers 
    * Standard Linux tools and zip

#### 2. Replace the config folder
Use the `config` folder provided in this repository instead of the one in the cloned SeBS directory.

The `config` folder in this repository contains 16 config files corresponding to 16 experiments (8 benchmarks for each architecture). 

#### 3. Install all benchmarks with support for AWS
Go to the SeBS directory in your terminal and run the following command:

    ./install.py --aws

#### 4. Activate a new python virtual environment
Run the following command:

    . python-venv/bin/activate

#### 5. Configure AWS Lambda
To use AWS, you must supply access and secret keys for a role that has the necessary permissions to handle Lambda functions and S3 resources.

Credentials can be provided by using the default environment variables specific to AWS:
    
    export AWS_ACCESS_KEY_ID=XXXX
    export AWS_SECRET_ACCESS_KEY=XXXX

or in the JSON configuration files:

    "deployment": {
        "name": "aws",
        "aws": {
            "region": "us-east-1",
            "lambda-role": "",
            "credentials": {
                "access_key": "YOUR AWS ACCESS KEY",
                "secret_key": "YOUR AWS SECRET KEY"
            }
        }
    }

#### 6. Run the experiments
To run an experiment, use the following command:

    ./sebs.py experiment invoke perf-cost --config config/example.json --output-dir desired_result_directory_name

**Notes:**
* Replace `example.json` with the name of the config file corresponding to the desired benchmark. 
* `desired_result_directory_name` is the name of the directory where the collected metrics of the experiments are saved.

#### 7. Process metrics
SeBS provides a command to summarize the metrics collected during all invocations (warm, cold, different memory sizes) in one CSV file. Run the following command to process the results:

    ./sebs.py experiment process perf-cost --config config/example.json --output-dir desired_result_directory_name


## Results
The `results` folder contains a zip file with all results collected by SeBS during our experiments.

* The `results.csv` file found under each benchmark's directory contains the summary as processed by SeBS. 
* For full details of the different invocations (including billing data), JSON files are provided for each configuration. 


## EvaluationScripts
`EvaluationScripts` is a Python project used to process and evaluate the collected data.

It consists of three packages:

* `cost`: Contains the scripts and the JSON results files used to estimate the cost data (compute cost and total cost). The folder also contains the diagrams and tables generated when running the scripts. 
* `perf`: Contains the scripts and the results files used to evaluate the performance-related metrics (memory, client times, cold start overheads, and execution times). It also contains the tables and diagrams generated by the scripts. 
* `perf_to_cost`: Contains the scripts used to calculate the performance-to-cost ratios. It also contains the generated plots and tables from the scripts.
